## 8.1 分词插件安装 ##
### IK分词插件 ###
每个节点ES的bin目录执行：
```
elasticsearch-plugin install \
https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.2/elasticsearch-analysis-ik-6.6.2.zip
```
重启ES生效，安装一个节点后可cp -r到其他节点plugins目录
### ansj分词插件 ###
和IK相同，地址改为：
```
elasticsearch-plugin install \
https://github.com/NLPchina/elasticsearch-analysis-ansj/releases/download/v6.6.2/elasticsearch-analysis-ansj-6.6.2.0-release.zip
```
### elasticsearch-analysis-pinyin拼音分词插件 ###
同上，地址改为：
elasticsearch-plugin install \
https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v6.6.2/elasticsearch-analysis-pinyin-6.6.2.zip

## 8.2 安装kibana ##
```
wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.2-linux-x86_64.tar.gz
tar -zxvf 解压
nohup ./bin/kibana &
```
默认5601端口,默认连localhost:9200的ES,配置可在config/kibana.yml修改

## 8.3 logstash安装及同步mysql数据到es ##
```
wget https://artifacts.elastic.co/downloads/logstash/logstash-6.6.2.zip
unzip 解压
```
配置conf文件，在config目录下新建一个logstash.conf文件，内容如下：
```
input {
    stdin {
    }
    jdbc {
      # mysql 数据库链接,test为数据库名
      jdbc_connection_string => "jdbc:mysql://localhost:3306/teacher_v2"
      # 用户名和密码
      jdbc_user => "root"
      jdbc_password => "123456"
      # 驱动
      jdbc_driver_library => "/home/zxy/es/logstash-6.6.2/config/mysql-connector-java-5.1.44.jar"
      # 驱动类名
      jdbc_driver_class => "com.mysql.jdbc.Driver"
      #处理中文乱码问题
      codec => plain { charset => "UTF-8"}
      #是否开启记录追踪
      record_last_run => "true"
      #是否需要追踪字段，如果为true，则需要指定tracking_column，默认是timestamp
      use_column_value => "true"
      #追踪的字段
      tracking_column => "update_time"
      #追踪字段的类型，目前只有数字numeric和时间类型timestamp，默认是numeric
      tracking_column_type => "timestamp"
      #是否每次清除last_run_metadata_path的内容
      clean_run => "false"
      #这里可以手动设置:sql_last_value的值，默认时间是1970-01-01，默认数字是0
      last_run_metadata_path => "/home/zxy/es/logstash-6.6.2/config/logstash_jdbc_last_run.txt"
      #开启分页查询
      jdbc_paging_enabled => "true"
      jdbc_page_size => "5000"  
      # 执行的sql 文件路径+名称
      statement_filepath => "/home/zxy/es/logstash-6.6.2/config/t_plan.sql"
      # 设置监听间隔  各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新
      schedule => "* * * * *"
      # 索引类型
      type => "t_plan_type"
    }
}

filter {
    json {
        source => "message"
        remove_field => ["message"]
    }
}

output {
    stdout {
        # JSON格式输出
        codec => json_lines
    }
    elasticsearch {
        hosts => "localhost:9200"
        # 定义索引名称
        index => "t_plan_idx"
        # 文档ID取mysql表的id字段
        document_id => "%{id}"
        # 重写模板
        template_overwrite => true
        template => "/home/zxy/es/logstash-6.6.2/template/logstash-ansj.json"
    }
}

```
conf文件配置中涉及到的t_plan.sql文件内容如下：
```
select * from t_plan where update_time > :sql_last_value
```
conf文件配置中涉及到的logstash-ansj.json文件内容如下：
```
{
    "mappings": {
        "doc": {
            "dynamic_templates": [
                {
                    "subject_fields": {
                        "mapping": {
                            "analyzer": "index_ansj_pinyin_analyzer",
                            "search_analyzer": "query_ansj_pinyin_analyzer",
                            "type": "text"
                        },
                        "match": "subject",
                        "match_mapping_type": "string"
                    }
                },
                {
                    "object_fields": {
                        "mapping": {
                            "analyzer": "index_ansj_pinyin_analyzer",
                            "search_analyzer": "query_ansj_pinyin_analyzer",
                            "type": "text"
                        },
                        "match": "object",
                        "match_mapping_type": "string"
                    }
                },
                {
                    "fname_fields": {
                        "mapping": {
                            "analyzer": "index_ansj_pinyin_analyzer",
                            "search_analyzer": "query_ansj_pinyin_analyzer",
                            "type": "text"
                        },
                        "match": "fname",
                        "match_mapping_type": "string"
                    }
                }
            ]
        }
    },
    "settings": {
        "analysis": {
            "analyzer": {
                "index_ansj_pinyin_analyzer": {
                    "filter": [
                        "my_pinyin",
                        "word_delimiter"
                    ],
                    "tokenizer": "index_ansj",
                    "type": "custom"
                },
                "query_ansj_pinyin_analyzer": {
                    "filter": [
                        "my_pinyin",
                        "word_delimiter"
                    ],
                    "tokenizer": "query_ansj",
                    "type": "custom"
                }
            },
            "filter": {
                "my_pinyin": {
                    "keep_first_letter": true,
                    "keep_full_pinyin": true,
                    "keep_none_chinese": true,
                    "keep_none_chinese_in_first_letter": true,
                    "keep_original": false,
                    "limit_first_letter_length": 16,
                    "lowercase": true,
                    "trim_whitespace": true,
                    "type": "pinyin"
                }
            }
        },
        "index.refresh_interval": "5s"
    },
    "template": "*",
    "version": 50001
}
```
该文件主要做了2件事情：     
1、定义分词器和过滤器：     
（1）定义拼音过滤器my_pinyin，使用到的是elasticsearch-analysis-pinyin拼音插件    
（2）定义index_ansj_pinyin_analyzer分词器和query_ansj_pinyin_analyzer分词器    
2、定义动态模板：     
（1）匹配subject、object、fname字段     
（2）将上面3个字段对应类型定义为text     
（3）使用index_ansj_pinyin_analyzer作为索引分词器，query_ansj_pinyin_analyzer作为搜索分词器    

启动logstash开始同步mysql数据到es：    
进入logstash的bin目录执行：    
./logstash -f ../config/logstash.conf